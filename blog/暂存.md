

#  kafka

1、创建topic：`bin/kafka-topics.sh --create --topic topicname --replication-factor 1 --partitions 1 --zookeeper localhost:2181 `

--topic 指定topic名字

--replication-factor 指定副本数，因为我的是集群环境，这里副本数就为3

--partitions 指定分区数，这个参数需要根据broker数和数据量决定，正常情况下，每个broker上两个partition最好

方法二：
开启自动创建配置：`auto.create.topics.enable=true `
使用程序直接往kafka中相应的topic发送数据，如果topic不存在就会按默认配置进行创建。

2、查看有哪些topic：`bin/kafka-topics.sh --list --zookeeper localhost:2181`

3、查看具体的topic详细信息：`bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --topic lx_test_topic --describe`

2、查询topic内容：

`./kafka-console-consumer.sh --bootstrap-server 127.0.0.1:21950 --topic s17_transcode_status --from-beginning`



查看topic 为 first的 详细信息

`./bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first`

往topic 为 first 的内部生产消息

`./bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first`

从topic 为first的内部消费消息

`./bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic first`



**kafka下载地址：**

 

**https://archive.apache.org/dist/kafka/**

# linux

http://39.100.154.238:21010/api/v1/file/download/fileId=11906238

netstat -anop|grep 29490

history |grep docker

ps -ef|grep mongo

netstat -nltp | grep 端口号

docker exec -it hwApp /bin/bash



/hwx/mongo/bin/mongo mongodb://eshk:BFmongo2019@127.0.0.1:27018/eshk



## 27个常用命令

https://www.jianshu.com/p/0056d671ea6d

## Ssh pem文件

https://blog.csdn.net/fdipzone/article/details/79763632

## jps

## scp

**一、scp使用说明：**

1、把本机的文件传给目的服务器：

```bash
scp get66.pcap root@192.168.1.147:/super1.
```

备注：把本机get66.pcap拷贝到147这台服务器的super目录下，需要提供147的密码

2、在本机上执行scp，把远端的服务器文件拷贝到本机上：

```bash
scp root@192.168.1.147:/super/dns.pcap /1.
```

备注：在本机上执行scp，把远端服务器的dns.pcap文件拷贝到本机的根目录下

3、拷贝目录下的所有文件：

```bash
scp -r /super/ root@192.168.1.145:/1.
```

备注：把/super/目录下的所有文件，拷贝到145服务器根目录下